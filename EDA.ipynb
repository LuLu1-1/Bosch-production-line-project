{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "datadir = r\"/content/gdrive/MyDrive/bosch_data\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = ['train_numeric', 'train_date', 'train_categorical']\n",
    "def explore_data_size(files):\n",
    "  # explore the size (rows, cols) of each file   \n",
    "  stats = []\n",
    "  for file_name in data_files:\n",
    "      cols = pd.read_csv(os.path.join(datadir,file_name+'.csv.zip'), nrows=1)\n",
    "      rows = pd.read_csv(os.path.join(datadir,file_name+'.csv.zip'), usecols=[\"Id\"])\n",
    "      stats.append({'File': file_name, 'Rows': rows.shape[0], 'Columns': cols.shape[1]})\n",
    "  # convert the result into a DataFrame so we can do plotting.\n",
    "  df = pd.DataFrame(stats, index=[\"File\"], columns=[\"Rows\", \"Columns\"])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_failure(date, res):\n",
    "  df = date.copy()\n",
    "  df.columns = pd.MultiIndex.from_tuples([tuple([int(a[1:]) for a in x[3:].split('_')]) for x in df.columns], names=['station', 'feature'])\n",
    "  df = df.groupby(level=0,axis=1).min()\n",
    "  stations = df.columns\n",
    "  date_res = pd.concat([df,res],axis=1)\n",
    "  fail_counts_per_station = date_res.loc[date_res.Response==1,stations].notna().sum(axis=0)\n",
    "  success_counts_per_station = date_res.loc[date_res.Response==0,stations].notna().sum(axis=0)\n",
    "  fail_rate_per_station = date_res.loc[date_res.Response==1,stations].notna().sum(axis=0)/date_res[stations].notna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_failure(date, res):\n",
    "  df = date.copy()\n",
    "  tuples = [tuple(int(a[1:]) for a in (x.split('_')[0],x.split('_')[2])) for x in df.columns]\n",
    "  new_columns = pd.MultiIndex.from_tuples(tuples, names=['line', 'feature'])\n",
    "  df = df.groupby(level=0,axis=1).min()\n",
    "  lines = df.columns\n",
    "  date_res = pd.concat([df,res],axis=1)\n",
    "  fail_counts_per_line = date_res.loc[date_res.Response==1,lines].notna().sum(axis=0)\n",
    "  success_counts_per_line = date_res.loc[date_res.Response==0,lines].notna().sum(axis=0)\n",
    "  fail_rate_per_line = date_res.loc[date_res.Response==1,stations].notna().sum(axis=0)/date_res[lines].notna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_station():\n",
    "  date = splits_cols('train_date',D=True)\n",
    "  line_station = {}\n",
    "  for i in date.Line.unique():\n",
    "    line_station[i] = date.loc[date.Line==i, 'Station'].unique()\n",
    "  return line_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_Tx_value(df):\n",
    "  tem = df.fillna(-99).values \n",
    "  cate_uniques = np.unique(tem[tem!=-99]) \n",
    "  return cate_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_sparse_rate(df,name):\n",
    "  sparse_rate = df.isnull().sum(axis = 1)/df.shape[1]\n",
    "  print(\"the average sparse rate of {} is: {:.2%}\".format(name, sparse_rate.mean()))\n",
    "  return sparse_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits_cols(file,D=False,r=False):\n",
    "  df = pd.read_csv(os.path.join(datadir,file+'.csv.zip'), index_col=0, compression='zip',nrows=0)\n",
    "  if r:\n",
    "    df.drop('Response',axis=1,inplace=True)\n",
    "  splits = [tuple([int(a[1:])  for a in x.split('_')]) for x in df.columns]\n",
    "  a = pd.DataFrame(splits,columns=['Line','Station','Feature'])\n",
    "  if D:\n",
    "    a['Feature'] = a['Feature'] - 1\n",
    "  \n",
    "  return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_station_structure():\n",
    "  date = splits_cols('train_date',D=True)\n",
    "  line_fcounts = date.groupby('Line')['Feature'].count()\n",
    "  station_fcounts = date.groupby('Station')['Feature'].count()\n",
    "  line_station = pd.concat([date.Station,date.Line],axis=1).drop_duplicates(keep='first',inplace=True).reset_index(drop=True,inplace=True)\n",
    "  line_stcounts = line_station.groupby('Line').count()\n",
    "  #plot: line-fcount,station-fount(line color),line-stationcount\n",
    "  #plot: station-feature scatter with line color, num and cat color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_network(df,res):\n",
    "  df = df.groupby(level=0,axis=1).min()          # date.min(axis=1, level=0)\n",
    "  stations = df.columns.tolist()\n",
    "  df['p'] = res.Response.apply(lambda x: 'p' if x==1 else np.nan)\n",
    "  df['n'] = res.Response.apply(lambda x: 'n' if x==0 else np.nan)\n",
    "  pos_idx = df[df.p=='p'].index   # 故障产品索引号\n",
    "  neg_idx = df[df.n=='n'].index    # 正常产品 索引号\n",
    "  stations.extend(['n', 'p'])\n",
    "  # station names\n",
    "  save_pickle(stations, 'stations.pickle')\n",
    "  # -------------station node status----------------------------------\n",
    "  node_weights = df[df.p=='p'].notna().sum(axis=0)/df.notna().sum(axis=0).to_dict()\n",
    "  #path_time = df.apply(lambda x: x.dropna().tolist(), axis=1)\n",
    "  total_samples = df.notna().sum(axis=0).to_dict()     # 通过每一个站点的数量\n",
    "  total_pos = df[df.p=='p'].notna().sum(axis=0).to_dict()\n",
    "  station_stat = {k: [node_weights[k], total_pos[k], total_samples[k]/df.shape[0], total_samples[k]] for k in node_weights.keys()}\n",
    "  station_stat = pd.DataFrame.from_dict(station_stat, orient='index')\n",
    "  station_stat.columns = ['error_rate', 'error_count', 'sample_rate', 'sample_count']\n",
    "  station_stat.index.name = 'station'\n",
    "  save_pickle(station_stat, 'node_station_stat.pickle')\n",
    "  del total_samples, total_pos, station_stat\n",
    "  gc.collect()\n",
    "  # -------------transition edge status------------------------------------\n",
    "  path_station = df.apply(lambda x: x.dropna().index.tolist(), axis=1)   # 获取通过的站点号\n",
    "  path_station_list = path_station.apply(lambda x: [(x[i], x[i+1]) for i in range(len(x)-1)]).values.tolist()\n",
    "  path_station_list_pos = path_station.loc[pos_idx].apply(lambda x: [(x[i], x[i+1]) for i in range(len(x)-1)]).values.tolist()\n",
    "  path_station_list = pd.Series([x for a in path_station_list for x in a])    # 将每行的传递tuple组分散到一起\n",
    "  path_station_list_pos = pd.Series([x for a in path_station_list_pos for x in a])\n",
    "  edges = path_station_list.value_counts().to_dict()\n",
    "  edges_pos = path_station_list_pos.value_counts().to_dict()\n",
    "  edges_pos_rate = {k: edges_pos[k]/edges[k] for k in edges_pos.keys()}\n",
    "  # Save transition error statistics\n",
    "  edges_stat = {k: [edges[k], edges_pos[k], edges_pos_rate[k]] for k in edges_pos.keys()}\n",
    "  edges_stat = pd.DataFrame.from_dict(edges_stat, orient='index')\n",
    "  edges_stat.columns = ['sample_count', 'error_count', 'error_rate']\n",
    "  edges_stat.index.name = 'transition'\n",
    "  save_pickle(edges_stat, 'edge_transition_stat.pickle')\n",
    "  del path_station, path_station_list, path_station_list_pos, edges_stat\n",
    "  gc.collect()\n",
    "  # -------------station path flow----------------------------------\n",
    "  station_path_flow = {}\n",
    "  for i in path_station.index:\n",
    "    station_path_flow[i] = '_'.join([str(x) for x in path_station[i]])\n",
    "  station_path_flow = pd.DataFrame(station_path_flow)\n",
    "  station_path_flow['response'] = station_path_flow[0].apply(lambda x: 0 if x.split('_')[-1]=='n' else 1)\n",
    "  station_path_flow[0] = station_path_flow[0].apply(lambda x: x[:-2])\n",
    "  station_path_flow.columns = ['flow', 'response']\n",
    "  station_path_flow_stat = \\\n",
    "    station_path_flow.groupby('flow', sort=False).agg([np.sum, np.mean, 'count']).sort_values(\n",
    "    by=[('response', 'sum'), ('response', 'mean'), ('response', 'count')], ascending=[False, False, False])\n",
    "  save_pickle(station_path_flow_stat, 'station_flow_error_stat.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.read_csv(os.path.join(datadir,'train_date'+'.csv.zip'), index_col=0, nrows=80000, compression='zip')\n",
    "response = pd.read_csv(os.path.join(datadir,'train_numeric'+'.csv.zip'), index_col=0, usecols=[0,969], nrows=80000, compression='zip')\n",
    "date_start = date.min(axis=1)\n",
    "date_end = date.max(axis=1)\n",
    "dt = pd.concat([date_start, date_end, date_end-date_start, response], axis=1)\n",
    "dt.columns = ['date_start', 'date_end', 'date_duration', 'Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dt.groupby('date_start').Response.agg(['mean', 'sum', 'count']).sort_values(['count'], ascending=False)\n",
    "x.sort_index(inplace=True)\n",
    "# interpolate time\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "t = np.arange(x.index.min(), x.index.max(), 0.01) # time axis\n",
    "x_counts = np.interp(t, x.index, x['count']) # interpolated part counts \n",
    "x_error_rate = np.interp(t, x.index, x['mean']) # interpolated part error rate\n",
    "x_errors = np.interp(t, x.index, x['sum']) # interpolated part error\n",
    "N = t.shape[0] # total number of time points\n",
    "T = 0.005 # time interval\n",
    "f = np.linspace(0.0, 1.0//(2.0*T), N//2) # frequency axis\n",
    "y_counts = fft(x_counts) # fft of x_counts\n",
    "y_errors = fft(x_errors) # fft of x_errors\n",
    "y_error_rate = fft(x_error_rate) # fft of x_error_rate\n",
    "plt.plot(f, 10*np.log10(2.0/N * np.abs(y_counts[:N//2]))) # plot frequency \n",
    "plt.axis([0, 0.33, -30.0, 1.0]) # zoom in\n",
    "plt.plot([0.0595, 0.0595], [-30, 1])\n",
    "plt.plot([0.119, 0.119], [-30, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_mod(date, period):  \n",
    "  tmp = (date['date_start'] % (period / 10)) * 10\n",
    "  date['date_start_mod1'] = tmp.values.astype(int)\n",
    "  a = date.groupby('date_start_mod1').Response.agg(['mean', 'sum', 'count'])\n",
    "  a.reset_index(inplace=True)\n",
    "  a.sort_values('date_start_mod1', ascending=True)\n",
    "  tmp = (date['date_end'] % (period / 10)) * 10\n",
    "  date['date_end_mod1'] = tmp.values.astype(int)\n",
    "  b = date.groupby('date_end_mod1').Response.agg(['mean', 'sum', 'count'])\n",
    "  b.reset_index(inplace=True)\n",
    "  b.sort_values('date_end_mod1', ascending=True)\n",
    "  fig1 = plt.figure(figsize=(14, 7))\n",
    "  plt.bar(a['date_start_mod1'], a['count'], alpha=0.7)\n",
    "  plt.bar(b['date_end_mod1'], b['count'], alpha=0.7)\n",
    "  # plt.plot(b['count']*10**-6)\n",
    "  plt.show()\n",
    "  fig2 = plt.figure(figsize=(14, 7))\n",
    "  plt.bar(a['date_start_mod1'], a['mean'], alpha=0.7)\n",
    "  plt.bar(b['date_end_mod1'], b['mean'], alpha=0.7)\n",
    "  # plt.plot(b['count']*10**-6)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stat(x, feat):\n",
    "  a = x.groupby(feat).Response.agg(['mean', 'sum', 'count']).sort_values(['count'], ascending=False)\n",
    "  a['confidence_interval'] = 1.96 * np.sqrt(a['mean'] * (1-a['mean']) / a['count'])\n",
    "  a['mean_low'] = a['mean'] - a['confidence_interval']\n",
    "  a['mean_high'] = a['mean'] + a['confidence_interval']\n",
    "  a.sort_values('count', ascending=False, inplace=True)\n",
    "  return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.read_csv(os.path.join(datadir,'train_date'+'.csv.zip'), index_col=0, nrows=2000, compression='zip')\n",
    "response = pd.read_csv(os.path.join(datadir,'train_numeric'+'.csv.zip'), index_col=0, use_columns=[0,969], compression='zip')\n",
    "tuples = [tuple([int(a[1:]) for a in x[3:].split('_')]) for x in date.columns]\n",
    "date.columns = pd.MultiIndex.from_tuples(tuples, names = ['station', 'feature'])\n",
    "station_time = pd.DataFrame()\n",
    "for c in date.columns.get_level_values(0).unique():\n",
    "  station_time['S{}_start'.format(c)] = date.min(axis=1).values\n",
    "  station_time['S{}_end'.format(c)] = date.max(axis=1).values\n",
    "  station_time['S{}_duration'.format(c)] = station_time['S{}_end'.format(c)] - station_time['S{}_start'.format(c)] \n",
    "station_time = station_time.join(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.read_csv(os.path.join(datadir,'train_date'+'.csv.zip'), index_col=0, nrows=2000, compression='zip')\n",
    "response = pd.read_csv(os.path.join(datadir,'train_numeric'+'.csv.zip'), index_col=0, use_columns=[0,969], compression='zip')\n",
    "tuples = [tuple(int(a[1:]) for a in (x.split('_')[0],x.split('_')[2])) for x in date.columns]\n",
    "date.columns = pd.MultiIndex.from_tuples(tuples, names = ['line', 'feature'])\n",
    "line_time = pd.DataFrame()\n",
    "for c in date.columns.get_level_values(0).unique():\n",
    "  line_time['L{}_start'.format(c)] = date.min(axis=1).values\n",
    "  line_time['L{}_end'.format(c)] = date.max(axis=1).values\n",
    "  line_time['L{}_duration'.format(c)] = line_time['L{}_end'.format(c)] - line_time['L{}_start'.format(c)] \n",
    "line_time = line_time.join(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
